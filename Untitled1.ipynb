{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afbb7c2b-9fd3-4fc8-8d9a-f7d26fe16a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a519743-33b7-4cb0-8dd7-5064cf275b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Folders created. Ensure you have copied model files to 'hf_models/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Directory structure ensure karna [cite: 49, 83]\n",
    "os.makedirs('agents', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('hf_models', exist_ok=True)\n",
    "\n",
    "print(\"âœ… Folders created. Ensure you have copied model files to 'hf_models/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdc93a30-b841-4fe4-9173-7c808429a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* /workspace/AAIPL/hf_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8e4334b-275e-4e58-a2df-2223053fbdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/question_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_model.py\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "class QuestionModel:\n",
    "    def __init__(self, model_path=\"hf_models\"):\n",
    "        # bfloat16 use kar rahe hain AMD hardware par speed ke liye\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_path, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def generate_question(self, topic):\n",
    "        # Strictly following the JSON format\n",
    "        prompt = f\"Topic: {topic}\\\\nGenerate a MCQ logic puzzle. Return ONLY valid JSON: {{\\\\\"topic\\\\\": \\\\\"{topic}\\\\\", \\\\\"question\\\\\": \\\\\"...\\\\\", \\\\\"choices\\\\\": [\\\\\"A) ..\\\\\", \\\\\"B) ..\\\\\", \\\\\"C) ..\\\\\", \\\\\"D) ..\\\\\"], \\\\\"answer\\\\\": \\\\\"A\\\\\", \\\\\"explanation\\\\\": \\\\\"...\\\\\"}}\"\n",
    "        # Token limit to stay under 13 seconds [cite: 77]\n",
    "        output = self.generator(prompt, max_new_tokens=250, do_sample=False)\n",
    "        return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b1d4944-86a1-414a-a744-8aa4c1d486fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/question_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_agent.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from agents.question_model import QuestionModel\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--num_questions\", type=int, default=20)\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = QuestionModel()\n",
    "    # Official topics for the challenge [cite: 30-34]\n",
    "    topics = [\"Syllogisms\", \"Seating Arrangements\", \"Blood Relations\", \"Alphanumeric Series\"]\n",
    "    questions = []\n",
    "\n",
    "    for i in range(args.num_questions):\n",
    "        topic = topics[i % len(topics)]\n",
    "        raw_output = model.generate_question(topic)\n",
    "        try:\n",
    "            start = raw_output.find('{')\n",
    "            end = raw_output.rfind('}') + 1\n",
    "            q_json = json.loads(raw_output[start:end])\n",
    "            questions.append(q_json)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with open(args.output_file, \"w\") as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "331ba33e-eaf1-4166-9364-b73b29bdce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self, model_path=\"hf_models\"):\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=model_path, \n",
    "            torch_dtype=torch.bfloat16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def answer_question(self, question_data):\n",
    "        # Only answer and reasoning keys required\n",
    "        prompt = f\"Question: {question_data['question']}\\\\nChoices: {question_data['choices']}\\\\nAnswer in JSON: {{\\\\\"answer\\\\\": \\\\\"Letter only\\\\\", \\\\\"reasoning\\\\\": \\\\\"brief explanation\\\\\"}}\"\n",
    "        # Fast inference to stay under 9 seconds [cite: 78]\n",
    "        output = self.generator(prompt, max_new_tokens=150, do_sample=False)\n",
    "        return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30766a1c-b755-4576-b777-0d83ccd45508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_agent.py\n",
    "import argparse\n",
    "import json\n",
    "from agents.answer_model import AnswerModel\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--output_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.input_file, \"r\") as f:\n",
    "        questions = json.load(f)\n",
    "\n",
    "    model = AnswerModel()\n",
    "    answers = []\n",
    "\n",
    "    for q in questions:\n",
    "        raw_output = model.answer_question(q)\n",
    "        try:\n",
    "            start = raw_output.find('{')\n",
    "            end = raw_output.rfind('}') + 1\n",
    "            a_json = json.loads(raw_output[start:end])\n",
    "            answers.append(a_json)\n",
    "        except:\n",
    "            answers.append({\"answer\": \"A\", \"reasoning\": \"Fallback\"})\n",
    "\n",
    "    with open(args.output_file, \"w\") as f:\n",
    "        json.dump(answers, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6fa858e-5fe7-46a5-bcdb-ddbb3e8c8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agents/question_model_llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_model_llama.py\n",
    "# Example placeholder for Unsloth Llama implementation\n",
    "from agents.question_model import QuestionModel\n",
    "class QuestionModelLlama(QuestionModel):\n",
    "    pass\n",
    "\n",
    "%%writefile agents/answer_model_llama.py\n",
    "# Example placeholder for Unsloth Llama implementation\n",
    "from agents.answer_model import AnswerModel\n",
    "class AnswerModelLlama(AnswerModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7fcdb76-b0d8-4603-9021-7b2aca445620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting qgen.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile qgen.yaml\n",
    "# Generation parameters for Q-agent [cite: 65, 74]\n",
    "max_tokens: 150 # Cumulatively for topic, question, choices, answer\n",
    "temperature: 0.7\n",
    "top_p: 0.9\n",
    "\n",
    "%%writefile agen.yaml\n",
    "# Generation parameters for A-agent [cite: 65, 78]\n",
    "max_tokens: 512 # reasoning limit [cite: 76]\n",
    "temperature: 0.1\n",
    "top_p: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65a51be5-490a-46a0-aeaa-c3c059b3c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/build_prompt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/build_prompt.py\n",
    "def build_q_prompt(topic):\n",
    "    return f\"Task: Generate a logic puzzle on {topic}. Format: JSON. Fields: topic, question, choices, answer, explanation.\"\n",
    "\n",
    "def build_a_prompt(question, choices):\n",
    "    return f\"Task: Solve this puzzle. Question: {question} Choices: {choices}. Format: JSON. Fields: answer, reasoning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f4eca27-4328-45d0-8492-ab0600008f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting git.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile git.sh\n",
    "#!/bin/bash\n",
    "# Script to push code to GitHub \n",
    "git add .\n",
    "git commit -m \"Final Submission AAIPL\"\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82b92804-e8ae-4f6b-8a96-3811654b82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 478, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1646, in _raise_on_head_call_error\n",
      "    raise LocalEntryNotFoundError(\n",
      "huggingface_hub.errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/workspace/AAIPL/agents/question_agent.py\", line 33, in <module>\n",
      "    main()\n",
      "  File \"/workspace/AAIPL/agents/question_agent.py\", line 13, in main\n",
      "    model = QuestionModel()\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AAIPL/agents/question_model.py\", line 10, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1078, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\", line 1288, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 552, in cached_files\n",
      "    raise OSError(\n",
      "OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    }
   ],
   "source": [
    "# Terminal mein ya notebook cell mein (!) laga kar run karein\n",
    "!python -m agents.question_agent \\\n",
    "    --output_file \"outputs/questions.json\" \\\n",
    "    --num_questions 5 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "633dc50f-d216-4f29-ade7-396bd732524b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AAIPL_10.108.10.21'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Folder Name Setup (Underscore separated IP as per rules)\u001b[39;00m\n\u001b[32m      4\u001b[39m folder_name = \u001b[33m\"\u001b[39m\u001b[33mAAIPL_10.108.10.21\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/agents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m os.makedirs(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/outputs\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m os.makedirs(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/hf_models\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:215\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'AAIPL_10.108.10.21'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Folder Name Setup (Underscore separated IP as per rules)\n",
    "folder_name = \"AAIPL_10.108.10.21\"\n",
    "os.makedirs(f\"{folder_name}/agents\", exist_ok=True)\n",
    "os.makedirs(f\"{folder_name}/outputs\", exist_ok=True)\n",
    "os.makedirs(f\"{folder_name}/hf_models\", exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Created folder structure for {folder_name}\")\n",
    "\n",
    "# 2. .gitignore create karein\n",
    "with open(f\"{folder_name}/.gitignore\", \"w\") as f:\n",
    "    f.write(\"hf_models/\\noutputs/\\n*.pyc\\n__pycache__/\\n.ipynb_checkpoints/\\n\")\n",
    "\n",
    "# 3. question_model.py (Mistral logic)\n",
    "q_model_code = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class QuestionModel:\n",
    "    def __init__(self, model_path=\"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def generate_question(self, topic):\n",
    "        prompt = f\"<s>[INST] Generate a puzzle about {topic}. Provide 4 choices A), B), C), D). Output ONLY JSON. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=512)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\"\"\"\n",
    "with open(f\"{folder_name}/agents/question_model.py\", \"w\") as f: f.write(q_model_code)\n",
    "\n",
    "# 4. answer_model.py (Mistral logic)\n",
    "a_model_code = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self, model_path=\"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def answer_question(self, question_data):\n",
    "        prompt = f\"<s>[INST] Solve: {question_data['question']} Choices: {question_data['choices']} Output ONLY JSON. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\"\"\"\n",
    "with open(f\"{folder_name}/agents/answer_model.py\", \"w\") as f: f.write(a_model_code)\n",
    "\n",
    "# 5. question_agent.py (Wrapper)\n",
    "q_agent_code = \"\"\"\n",
    "import argparse, json, os\n",
    "from agents.question_model import QuestionModel\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(); parser.add_argument(\"--output_file\", default=\"outputs/questions.json\"); parser.add_argument(\"--num_questions\", type=int, default=20); args = parser.parse_args()\n",
    "    model = QuestionModel(); topics = [\"Syllogisms\", \"Seating Arrangements\", \"Blood Relations\", \"Mixed Series\"]; questions = []\n",
    "    for i in range(args.num_questions):\n",
    "        raw = model.generate_question(topics[i % 4])\n",
    "        try: questions.append(json.loads(raw[raw.find('{'):raw.rfind('}')+1]))\n",
    "        except: pass\n",
    "    with open(args.output_file, \"w\") as f: json.dump(questions, f, indent=4)\n",
    "if __name__ == \"__main__\": main()\n",
    "\"\"\"\n",
    "with open(f\"{folder_name}/agents/question_agent.py\", \"w\") as f: f.write(q_agent_code)\n",
    "\n",
    "# 6. answer_agent.py (Wrapper)\n",
    "a_agent_code = \"\"\"\n",
    "import argparse, json, os\n",
    "from agents.answer_model import AnswerModel\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(); parser.add_argument(\"--input_file\", default=\"outputs/filtered_questions.json\"); parser.add_argument(\"--output_file\", default=\"outputs/answers.json\"); args = parser.parse_args()\n",
    "    with open(args.input_file, \"r\") as f: questions = json.load(f)\n",
    "    model = AnswerModel(); answers = []\n",
    "    for q in questions:\n",
    "        raw = model.answer_question(q)\n",
    "        try: answers.append(json.loads(raw[raw.find('{'):raw.rfind('}')+1]))\n",
    "        except: answers.append({\"answer\": \"A\", \"reasoning\": \"Error\"})\n",
    "    with open(args.output_file, \"w\") as f: json.dump(answers, f, indent=4)\n",
    "if __name__ == \"__main__\": main()\n",
    "\"\"\"\n",
    "with open(f\"{folder_name}/agents/answer_agent.py\", \"w\") as f: f.write(a_agent_code)\n",
    "\n",
    "print(\"ðŸš€ All 4 agent files and structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afb4a2-db8c-4ff6-99c8-f28d5b993dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356ae02-68b9-4588-98f4-7d5bf8a13752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
