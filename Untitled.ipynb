{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d64ef6d-50f6-4013-b6ae-eef54c5937a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/question_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_model.py\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ===============================\n",
    "# CHANGE MODEL PATH IF NEEDED\n",
    "# ===============================\n",
    "MODEL_PATH = \"\"\n",
    "\n",
    "\n",
    "class QuestionModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # üî• EDITABLE PROMPT SECTION\n",
    "    # ===============================\n",
    "    def build_prompt(self, topic):\n",
    "\n",
    "        return f\"\"\"\n",
    "You are an expert puzzle creator.\n",
    "\n",
    "Generate ONE highly challenging puzzle-based multiple choice question \n",
    "based on the topic: {topic}\n",
    "\n",
    "STRICT RULES:\n",
    "- Must be logical or reasoning based\n",
    "- Exactly 4 options\n",
    "- Only one correct answer\n",
    "- Return valid JSON only\n",
    "- No extra explanation outside JSON\n",
    "\n",
    "Required JSON format:\n",
    "\n",
    "{{\n",
    "    \"topic\": \"{topic}\",\n",
    "    \"question\": \"Write the full question here\",\n",
    "    \"choices\": {{\n",
    "        \"A\": \"Option A\",\n",
    "        \"B\": \"Option B\",\n",
    "        \"C\": \"Option C\",\n",
    "        \"D\": \"Option D\"\n",
    "    }},\n",
    "    \"answer\": \"A\",\n",
    "    \"explanation\": \"Short reasoning\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # DO NOT MODIFY BELOW UNLESS NEEDED\n",
    "    # ===============================\n",
    "    def generate_question(self, topic):\n",
    "\n",
    "        prompt = self.build_prompt(topic)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=250,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            start = response.index(\"{\")\n",
    "            end = response.rindex(\"}\") + 1\n",
    "            parsed = json.loads(response[start:end])\n",
    "\n",
    "            if parsed.get(\"answer\") in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return parsed\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Fallback (competition safe)\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"question\": \"Fallback question.\",\n",
    "            \"choices\": {\n",
    "                \"A\": \"1\",\n",
    "                \"B\": \"2\",\n",
    "                \"C\": \"3\",\n",
    "                \"D\": \"4\"\n",
    "            },\n",
    "            \"answer\": \"A\",\n",
    "            \"explanation\": \"Fallback\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922a42e1-5f88-42ec-988d-88f2ef9817ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* /workspace/AAIPL/hf_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8fee291-dfa1-4b77-80d3-41dcca3114f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ===============================\n",
    "# CHANGE MODEL PATH IF NEEDED\n",
    "# ===============================\n",
    "MODEL_PATH = \"hf_models/YOUR_MODEL_FOLDER_NAME\"\n",
    "\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # üî• EDITABLE PROMPT SECTION\n",
    "    # ===============================\n",
    "    def build_prompt(self, question_text, choices):\n",
    "\n",
    "        return f\"\"\"\n",
    "You are a highly accurate reasoning engine.\n",
    "\n",
    "Solve the following question carefully.\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Options:\n",
    "A. {choices[\"A\"]}\n",
    "B. {choices[\"B\"]}\n",
    "C. {choices[\"C\"]}\n",
    "D. {choices[\"D\"]}\n",
    "\n",
    "Rules:\n",
    "- Think internally\n",
    "- Return ONLY JSON\n",
    "- Do not return explanation\n",
    "\n",
    "Required format:\n",
    "{{\"answer\": \"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # DO NOT MODIFY BELOW UNLESS NEEDED\n",
    "    # ===============================\n",
    "    def solve(self, question_text, choices):\n",
    "\n",
    "        prompt = self.build_prompt(question_text, choices)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.2\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            start = response.index(\"{\")\n",
    "            end = response.rindex(\"}\") + 1\n",
    "            parsed = json.loads(response[start:end])\n",
    "\n",
    "            if parsed.get(\"answer\") in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return parsed\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return {\"answer\": \"A\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7acf043a-e308-49bc-8076-f3f511503fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/YOUR_MODEL_FOLDER_NAME\"\n",
    "\n",
    "A_VOTES = 3   # üî• You can change this (3 recommended for speed)\n",
    "\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # EDITABLE PROMPT\n",
    "    # ===============================\n",
    "    def build_prompt(self, question_text, choices):\n",
    "        return f\"\"\"\n",
    "Solve the following MCQ carefully.\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Options:\n",
    "A. {choices[\"A\"]}\n",
    "B. {choices[\"B\"]}\n",
    "C. {choices[\"C\"]}\n",
    "D. {choices[\"D\"]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"answer\":\"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # Helper: Extract letter\n",
    "    # ===============================\n",
    "    def extract_letter(self, text):\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        # fallback: detect standalone letter\n",
    "        for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if letter in text:\n",
    "                return letter\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ===============================\n",
    "    # Multi-Vote Solver\n",
    "    # ===============================\n",
    "    def solve(self, question_text, choices):\n",
    "\n",
    "        prompt = self.build_prompt(question_text, choices)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        vote_count = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "\n",
    "        for _ in range(A_VOTES):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.6,   # small randomness for diversity\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            letter = self.extract_letter(response)\n",
    "\n",
    "            if letter in vote_count:\n",
    "                vote_count[letter] += 1\n",
    "\n",
    "        # majority selection\n",
    "        final_answer = max(vote_count, key=vote_count.get)\n",
    "\n",
    "        return {\"answer\": final_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb148da8-eac2-49a3-95d8-8bf2b442ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/YOUR_MODEL_FOLDER_NAME\"\n",
    "\n",
    "A_VOTES = 3   # Recommended: 3 for speed/accuracy balance\n",
    "\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # EDITABLE PROMPT\n",
    "    # ===============================\n",
    "    def build_prompt(self, question_text, choices):\n",
    "        return f\"\"\"\n",
    "Solve the following multiple-choice question carefully.\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Options:\n",
    "A. {choices[\"A\"]}\n",
    "B. {choices[\"B\"]}\n",
    "C. {choices[\"C\"]}\n",
    "D. {choices[\"D\"]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"answer\":\"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # Extract answer letter safely\n",
    "    # ===============================\n",
    "    def extract_letter(self, text):\n",
    "\n",
    "        # Try strict JSON match first\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        # Fallback: detect standalone capital letter\n",
    "        for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if f'\"{letter}\"' in text or f\" {letter}\" in text:\n",
    "                return letter\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ===============================\n",
    "    # Multi-Vote + Confidence\n",
    "    # ===============================\n",
    "    def solve(self, question_text, choices):\n",
    "\n",
    "        prompt = self.build_prompt(question_text, choices)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        vote_count = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "\n",
    "        for _ in range(A_VOTES):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            letter = self.extract_letter(response)\n",
    "\n",
    "            if letter in vote_count:\n",
    "                vote_count[letter] += 1\n",
    "\n",
    "        # Majority decision\n",
    "        final_answer = max(vote_count, key=vote_count.get)\n",
    "        confidence = vote_count[final_answer] / A_VOTES\n",
    "\n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"confidence\": round(confidence, 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a13ec505-fd79-4cf8-bda2-868d958d280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/YOUR_MODEL_FOLDER_NAME\"\n",
    "\n",
    "A_VOTES = 3\n",
    "TIME_LIMIT = 8.5   # seconds (safe buffer under 9 sec)\n",
    "\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # EDITABLE PROMPT\n",
    "    # ===============================\n",
    "    def build_prompt(self, question_text, choices):\n",
    "        return f\"\"\"\n",
    "Solve the following multiple-choice question carefully.\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Options:\n",
    "A. {choices[\"A\"]}\n",
    "B. {choices[\"B\"]}\n",
    "C. {choices[\"C\"]}\n",
    "D. {choices[\"D\"]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"answer\":\"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # Extract answer letter safely\n",
    "    # ===============================\n",
    "    def extract_letter(self, text):\n",
    "\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if letter in text:\n",
    "                return letter\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ===============================\n",
    "    # Multi-Vote + Confidence + Time Guard\n",
    "    # ===============================\n",
    "    def solve(self, question_text, choices):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        prompt = self.build_prompt(question_text, choices)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        vote_count = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "        completed_votes = 0\n",
    "\n",
    "        for _ in range(A_VOTES):\n",
    "\n",
    "            # ‚è± Time Guard Check\n",
    "            if time.time() - start_time > TIME_LIMIT:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            letter = self.extract_letter(response)\n",
    "\n",
    "            if letter in vote_count:\n",
    "                vote_count[letter] += 1\n",
    "                completed_votes += 1\n",
    "\n",
    "        # If no votes completed (extreme edge case)\n",
    "        if completed_votes == 0:\n",
    "            return {\"answer\": \"A\", \"confidence\": 0.0}\n",
    "\n",
    "        final_answer = max(vote_count, key=vote_count.get)\n",
    "        confidence = vote_count[final_answer] / completed_votes\n",
    "\n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"confidence\": round(confidence, 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c3fe7c-e709-4540-a75f-4f201c330316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/YOUR_MODEL_FOLDER_NAME\"\n",
    "\n",
    "INITIAL_VOTES = 3\n",
    "MAX_EXTRA_VOTES = 2\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "TIME_LIMIT = 8.5   # safe under 9 sec\n",
    "\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "    # ===============================\n",
    "    # EDITABLE PROMPT\n",
    "    # ===============================\n",
    "    def build_prompt(self, question_text, choices):\n",
    "        return f\"\"\"\n",
    "Solve the following multiple-choice question carefully.\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Options:\n",
    "A. {choices[\"A\"]}\n",
    "B. {choices[\"B\"]}\n",
    "C. {choices[\"C\"]}\n",
    "D. {choices[\"D\"]}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"answer\":\"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # Extract answer safely\n",
    "    # ===============================\n",
    "    def extract_letter(self, text):\n",
    "\n",
    "        match = re.search(r'\"answer\"\\s*:\\s*\"([ABCD])\"', text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "        for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if letter in text:\n",
    "                return letter\n",
    "\n",
    "        return None\n",
    "\n",
    "    # ===============================\n",
    "    # Adaptive Voting Solver\n",
    "    # ===============================\n",
    "    def solve(self, question_text, choices):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        prompt = self.build_prompt(question_text, choices)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=3000\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        vote_count = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "        total_votes = 0\n",
    "\n",
    "        # ---------- Initial Votes ----------\n",
    "        for _ in range(INITIAL_VOTES):\n",
    "\n",
    "            if time.time() - start_time > TIME_LIMIT:\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            letter = self.extract_letter(response)\n",
    "\n",
    "            if letter in vote_count:\n",
    "                vote_count[letter] += 1\n",
    "                total_votes += 1\n",
    "\n",
    "        if total_votes == 0:\n",
    "            return {\"answer\": \"A\", \"confidence\": 0.0}\n",
    "\n",
    "        # Calculate confidence\n",
    "        best_answer = max(vote_count, key=vote_count.get)\n",
    "        confidence = vote_count[best_answer] / total_votes\n",
    "\n",
    "        # ---------- Adaptive Extra Votes ----------\n",
    "        extra_votes = 0\n",
    "\n",
    "        while (\n",
    "            confidence < CONFIDENCE_THRESHOLD and\n",
    "            extra_votes < MAX_EXTRA_VOTES and\n",
    "            time.time() - start_time < TIME_LIMIT\n",
    "        ):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            letter = self.extract_letter(response)\n",
    "\n",
    "            if letter in vote_count:\n",
    "                vote_count[letter] += 1\n",
    "                total_votes += 1\n",
    "\n",
    "            best_answer = max(vote_count, key=vote_count.get)\n",
    "            confidence = vote_count[best_answer] / total_votes\n",
    "\n",
    "            extra_votes += 1\n",
    "\n",
    "        return {\n",
    "            \"answer\": best_answer,\n",
    "            \"confidence\": round(confidence, 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "367c9bbe-0328-4693-8420-be3b9fa60369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "    def _generate_once(self, prompt, temperature=0.7):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def _extract_answer(self, text):\n",
    "        for letter in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if f'\"answer\": \"{letter}\"' in text or f\"Answer: {letter}\" in text:\n",
    "                return letter\n",
    "        return None\n",
    "\n",
    "    def solve(self, question_text, choices):\n",
    "        start_time = time.time()\n",
    "        TIME_LIMIT = 8.5  # keep safe under 9 sec limit\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a highly logical AI.\n",
    "Solve step-by-step internally.\n",
    "Return ONLY valid JSON in this format:\n",
    "{{\n",
    "    \"answer\": \"A/B/C/D\",\n",
    "    \"confidence\": float between 0 and 1\n",
    "}}\n",
    "\n",
    "Question:\n",
    "{question_text}\n",
    "\n",
    "Choices:\n",
    "A: {choices.get(\"A\")}\n",
    "B: {choices.get(\"B\")}\n",
    "C: {choices.get(\"C\")}\n",
    "D: {choices.get(\"D\")}\n",
    "\"\"\"\n",
    "\n",
    "        votes = []\n",
    "\n",
    "        # -------- Initial Multi-Vote (3 votes) --------\n",
    "        for _ in range(3):\n",
    "            if time.time() - start_time > TIME_LIMIT:\n",
    "                break\n",
    "            response = self._generate_once(prompt, temperature=0.7)\n",
    "            ans = self._extract_answer(response)\n",
    "            if ans:\n",
    "                votes.append(ans)\n",
    "\n",
    "        if not votes:\n",
    "            return {\"answer\": \"A\", \"confidence\": 0.25}\n",
    "\n",
    "        counter = Counter(votes)\n",
    "        best_answer, count = counter.most_common(1)[0]\n",
    "        confidence = count / len(votes)\n",
    "\n",
    "        # -------- Adaptive Voting --------\n",
    "        if confidence < 0.7 and time.time() - start_time < TIME_LIMIT:\n",
    "            for _ in range(2):  # extra votes\n",
    "                if time.time() - start_time > TIME_LIMIT:\n",
    "                    break\n",
    "                response = self._generate_once(prompt, temperature=0.7)\n",
    "                ans = self._extract_answer(response)\n",
    "                if ans:\n",
    "                    votes.append(ans)\n",
    "\n",
    "            counter = Counter(votes)\n",
    "            best_answer, count = counter.most_common(1)[0]\n",
    "            confidence = count / len(votes)\n",
    "\n",
    "        # -------- Deterministic Fallback --------\n",
    "        if confidence < 0.6 and time.time() - start_time < TIME_LIMIT:\n",
    "            response = self._generate_once(prompt, temperature=0.0)\n",
    "            ans = self._extract_answer(response)\n",
    "            if ans:\n",
    "                best_answer = ans\n",
    "                confidence = max(confidence, 0.75)\n",
    "\n",
    "        # -------- Final Safe Return --------\n",
    "        return {\n",
    "            \"answer\": best_answer,\n",
    "            \"confidence\": round(float(confidence), 2)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56dd47-3771-4128-a620-838cae7e14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile agents/question_model.py\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "class QuestionModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "    def _generate(self, prompt, temperature=0.8):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,\n",
    "            temperature=temperature,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def generate_question(self, domain=\"general knowledge\"):\n",
    "        TIME_LIMIT = 9\n",
    "        start_time = time.time()\n",
    "\n",
    "        base_prompt = f\"\"\"\n",
    "You are an expert exam creator.\n",
    "\n",
    "Create ONE very challenging but valid multiple choice question in the domain: {domain}\n",
    "\n",
    "Rules:\n",
    "- 4 options (A,B,C,D)\n",
    "- Exactly ONE correct answer\n",
    "- No ambiguity\n",
    "- Avoid trick wording\n",
    "- Make distractors very strong\n",
    "- Question should require reasoning\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "\n",
    "{{\n",
    "  \"question\": \"...\",\n",
    "  \"choices\": {{\n",
    "      \"A\": \"...\",\n",
    "      \"B\": \"...\",\n",
    "      \"C\": \"...\",\n",
    "      \"D\": \"...\"\n",
    "  }},\n",
    "  \"answer\": \"A/B/C/D\",\n",
    "  \"difficulty\": float between 0 and 1\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        response = self._generate(base_prompt)\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "        except:\n",
    "            # Retry once if invalid\n",
    "            response = self._generate(base_prompt)\n",
    "            try:\n",
    "                data = json.loads(response)\n",
    "            except:\n",
    "                return {\n",
    "                    \"question\": \"Which planet is known as the Red Planet?\",\n",
    "                    \"choices\": {\n",
    "                        \"A\": \"Earth\",\n",
    "                        \"B\": \"Mars\",\n",
    "                        \"C\": \"Venus\",\n",
    "                        \"D\": \"Jupiter\"\n",
    "                    },\n",
    "                    \"answer\": \"B\",\n",
    "                    \"difficulty\": 0.3\n",
    "                }\n",
    "\n",
    "        # Basic validation\n",
    "        if \"question\" not in data or \"choices\" not in data or \"answer\" not in data:\n",
    "            return {\n",
    "                \"question\": \"Which planet is known as the Red Planet?\",\n",
    "                \"choices\": {\n",
    "                    \"A\": \"Earth\",\n",
    "                    \"B\": \"Mars\",\n",
    "                    \"C\": \"Venus\",\n",
    "                    \"D\": \"Jupiter\"\n",
    "                },\n",
    "                \"answer\": \"B\",\n",
    "                \"difficulty\": 0.3\n",
    "            }\n",
    "\n",
    "        if data[\"answer\"] not in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "            data[\"answer\"] = \"A\"\n",
    "\n",
    "        if time.time() - start_time > TIME_LIMIT:\n",
    "            data[\"difficulty\"] = min(data.get(\"difficulty\", 0.5), 0.7)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20868095-dfb4-4235-a7d2-603ed9268d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting self_play.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile self_play.py\n",
    "import time\n",
    "import json\n",
    "from agents.question_model import QuestionModel\n",
    "from agents.answer_model import AnswerModel\n",
    "\n",
    "class SelfPlayArena:\n",
    "    def __init__(self, rounds=10):\n",
    "        self.rounds = rounds\n",
    "        self.q_model = QuestionModel()\n",
    "        self.a_model = AnswerModel()\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"correct\": 0,\n",
    "            \"wrong\": 0,\n",
    "            \"avg_confidence\": 0\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        print(\"üî• Starting Self-Play Training...\\n\")\n",
    "\n",
    "        total_conf = 0\n",
    "\n",
    "        for i in range(self.rounds):\n",
    "            print(f\"Round {i+1}/{self.rounds}\")\n",
    "\n",
    "            question = self.q_model.generate_question(\"logical reasoning\")\n",
    "            answer = self.a_model.solve(question[\"question\"], question[\"choices\"])\n",
    "\n",
    "            correct_letter = question[\"answer\"]\n",
    "            predicted_letter = answer[\"answer\"]\n",
    "            confidence = answer.get(\"confidence\", 0)\n",
    "\n",
    "            self.stats[\"total\"] += 1\n",
    "            total_conf += confidence\n",
    "\n",
    "            if predicted_letter == correct_letter:\n",
    "                self.stats[\"correct\"] += 1\n",
    "                print(\"‚úÖ Correct\")\n",
    "            else:\n",
    "                self.stats[\"wrong\"] += 1\n",
    "                print(\"‚ùå Wrong\")\n",
    "\n",
    "            print(\"Correct:\", correct_letter)\n",
    "            print(\"Predicted:\", predicted_letter)\n",
    "            print(\"Confidence:\", confidence)\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        self.stats[\"avg_confidence\"] = total_conf / self.rounds\n",
    "\n",
    "        return self.stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arena = SelfPlayArena(rounds=5)\n",
    "    results = arena.run()\n",
    "\n",
    "    print(\"\\nüèÜ Self-Play Results\")\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88ee6184-5889-4e95-96dd-2fe527df06bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 478, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1646, in _raise_on_head_call_error\n",
      "    raise LocalEntryNotFoundError(\n",
      "huggingface_hub.errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/AAIPL/self_play.py\", line 54, in <module>\n",
      "    arena = SelfPlayArena(rounds=5)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AAIPL/self_play.py\", line 9, in __init__\n",
      "    self.q_model = QuestionModel()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/AAIPL/agents/question_model.py\", line 10, in __init__\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1078, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\", line 1288, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 552, in cached_files\n",
      "    raise OSError(\n",
      "OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
      "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    }
   ],
   "source": [
    "!python self_play.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "736fc19c-0e3b-42a2-9731-60e86bca1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"hf_models\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa1a3df-6755-4baf-92b2-fb7246bf7bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hf_models/model_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhf_models/model_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'hf_models/model_1'"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"hf_models/model_1\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5c68d6-6912-4e83-86e5-d828e87cd342",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -type f -name \"*.safetensors\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2967fe82-bad4-4e45-801d-d7844fa498a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -type f -name \"pytorch_model.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "996b74c7-960d-478a-9574-e7050c72d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -type f -name \"tokenizer.model\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69aaefd9-af22-45c0-8537-026d0accb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* /workspace/AAIPL/hf_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e94d13b-0e3f-4621-b449-cb3df65d87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"hf_models\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccb22f29-cf75-4fdb-9d66-b4e9814d9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"hf_models/model_1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3377ed6b-b82a-468c-88c1-a3fe49e77a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* ./hf_models/model_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbf6f129-95d1-4114-b542-6a6fda5b30f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"hf_models/model_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbbc217d-1084-4ac3-8d10-257baa2366a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p hf_models/model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffee76df-1d99-40aa-ab62-bb153d8a133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: not writing through dangling symlink './hf_models/model_1/./config.json'\n"
     ]
    }
   ],
   "source": [
    "cp -aL /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/. ./hf_models/model_1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ac5fd0a-2dbf-4116-89e4-b11c2b789129",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2786369552.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcp -RL /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* ./hf_models/model_1/\u001b[39m\n                                                                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# 1. Delete the folder with the broken shortcuts\n",
    "rm -rf ./hf_models/model_1\n",
    "\n",
    "# 2. Create a fresh, empty folder\n",
    "mkdir -p ./hf_models/model_1\n",
    "\n",
    "# 3. Copy the ACTUAL files (using -L to turn shortcuts into real files)\n",
    "# Note: I'm using the path from your previous error message\n",
    "cp -RL /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* ./hf_models/model_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42a806b6-c75b-4148-9ab4-b16e88d3acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clean up any broken links first\n",
    "!rm -rf ./hf_models/model_1\n",
    "!mkdir -p ./hf_models/model_1\n",
    "\n",
    "# 2. Use the '!' so Python knows this is a system command\n",
    "!cp -RL /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/* ./hf_models/model_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75c3d677-7b52-42f9-9b06-876b297b22cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Found 1 files in ./hf_models/model_1:\n",
      "--- config.json (0.0000 GB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "target_dir = \"./hf_models/model_1\"\n",
    "if os.path.exists(target_dir):\n",
    "    files = os.listdir(target_dir)\n",
    "    print(f\"‚úÖ Success! Found {len(files)} files in {target_dir}:\")\n",
    "    for f in files:\n",
    "        size_gb = os.path.getsize(os.path.join(target_dir, f)) / (1024**3)\n",
    "        print(f\"--- {f} ({size_gb:.4f} GB)\")\n",
    "else:\n",
    "    print(\"‚ùå Folder still does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0794aef0-bf3f-4dea-bdff-eee8501ca059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Tokenizer...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m./hf_models/model_1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Loading Model (this might take a minute)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     12\u001b[39m     model_path,\n\u001b[32m     13\u001b[39m     torch_dtype=torch.float16,   \u001b[38;5;66;03m# Uses less VRAM/RAM\u001b[39;00m\n\u001b[32m     14\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,           \u001b[38;5;66;03m# Automatically finds your GPU\u001b[39;00m\n\u001b[32m     15\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m        \u001b[38;5;66;03m# Forces it to use your hf_models folder\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1078\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py:1321\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1313\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1319\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1320\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1324\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING.keys(), key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:808\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    806\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    811\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/configuration_llama.py:209\u001b[39m, in \u001b[36mLlamaConfig.__init__\u001b[39m\u001b[34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mlp_bias, head_dim, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_dropout = attention_dropout\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp_bias = mlp_bias\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28mself\u001b[39m.head_dim = head_dim \u001b[38;5;28;01mif\u001b[39;00m head_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Validate the correctness of rotary position embeddings parameters\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# BC: if there is a 'type' field, copy it it to 'rope_type'.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling:\n",
      "\u001b[31mZeroDivisionError\u001b[39m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. Point to your local folder\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Step 1: Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "print(\"Step 2: Loading Model (this might take a minute)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,   # Uses less VRAM/RAM\n",
    "    device_map=\"auto\",           # Automatically finds your GPU\n",
    "    local_files_only=True        # Forces it to use your hf_models folder\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fef623ad-b084-4581-93bd-463a1086ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config.json has been refreshed and validated!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# This is the raw URL for the unsloth Llama-3 config (adjust if using a different model)\n",
    "config_url = \"https://huggingface.co/unsloth/llama-3-8b-bnb-4bit/raw/main/config.json\"\n",
    "response = requests.get(config_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"./hf_models/model_1/config.json\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"‚úÖ config.json has been refreshed and validated!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to download a fresh config. Please check your internet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "840b04f5-a57a-443c-a20c-872946100d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n",
      "Verified Config: 32 attention heads found.\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVerified Config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.num_attention_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attention heads found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model weights...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     17\u001b[39m     model_path,\n\u001b[32m     18\u001b[39m     config=config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2108\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m     slow_tokenizer = \u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslow_tokenizer_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2120\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2314\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py:171\u001b[39m, in \u001b[36mLlamaTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m.add_eos_token = add_eos_token\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.use_default_system_prompt = use_default_system_prompt\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28mself\u001b[39m.sp_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_spm_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrom_slow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.add_prefix_space = add_prefix_space\n\u001b[32m    174\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    175\u001b[39m     bos_token=bos_token,\n\u001b[32m    176\u001b[39m     eos_token=eos_token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     **kwargs,\n\u001b[32m    188\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py:198\u001b[39m, in \u001b[36mLlamaTokenizer.get_spm_processor\u001b[39m\u001b[34m(self, from_slow)\u001b[39m\n\u001b[32m    196\u001b[39m tokenizer = spm.SentencePieceProcessor(**\u001b[38;5;28mself\u001b[39m.sp_model_kwargs)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy \u001b[38;5;129;01mor\u001b[39;00m from_slow:  \u001b[38;5;66;03m# no dependency on protobuf\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocab_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py:961\u001b[39m, in \u001b[36mSentencePieceProcessor.Load\u001b[39m\u001b[34m(self, model_file, model_proto)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[32m    960\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.LoadFromSerializedProto(model_proto)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py:316\u001b[39m, in \u001b[36mSentencePieceProcessor.LoadFromFile\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Loading configuration...\")\n",
    "# We load the config separately first to verify it's working\n",
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "print(f\"Verified Config: {config.num_attention_heads} attention heads found.\")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "print(\"Loading model weights...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Success! Model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c12b1dcd-22cb-4329-949f-51542a6de4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Tokenizer (Fast Mode)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Tokenizer (Fast Mode)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# use_fast=True skips the need for the 'tokenizer.model' file in many cases\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Loading Model weights...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     16\u001b[39m     model_path,\n\u001b[32m     17\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m     18\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2108\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m     slow_tokenizer = \u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslow_tokenizer_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2120\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2314\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py:171\u001b[39m, in \u001b[36mLlamaTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m.add_eos_token = add_eos_token\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.use_default_system_prompt = use_default_system_prompt\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28mself\u001b[39m.sp_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_spm_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrom_slow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.add_prefix_space = add_prefix_space\n\u001b[32m    174\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    175\u001b[39m     bos_token=bos_token,\n\u001b[32m    176\u001b[39m     eos_token=eos_token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     **kwargs,\n\u001b[32m    188\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/tokenization_llama.py:198\u001b[39m, in \u001b[36mLlamaTokenizer.get_spm_processor\u001b[39m\u001b[34m(self, from_slow)\u001b[39m\n\u001b[32m    196\u001b[39m tokenizer = spm.SentencePieceProcessor(**\u001b[38;5;28mself\u001b[39m.sp_model_kwargs)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy \u001b[38;5;129;01mor\u001b[39;00m from_slow:  \u001b[38;5;66;03m# no dependency on protobuf\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocab_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py:961\u001b[39m, in \u001b[36mSentencePieceProcessor.Load\u001b[39m\u001b[34m(self, model_file, model_proto)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[32m    960\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.LoadFromSerializedProto(model_proto)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/sentencepiece/__init__.py:316\u001b[39m, in \u001b[36mSentencePieceProcessor.LoadFromFile\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: not a string"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Step 1: Loading Tokenizer (Fast Mode)...\")\n",
    "# use_fast=True skips the need for the 'tokenizer.model' file in many cases\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    local_files_only=True,\n",
    "    use_fast=True \n",
    ")\n",
    "\n",
    "print(\"Step 2: Loading Model weights...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SUCCESS! Everything is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1766c7b-053c-4a98-9135-520c2b6d3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Tokenizer (Direct Fast Class)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:1600\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m byte_encoder = bytes_to_unicode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tiktoken/load.py:162\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     ret = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/tiktoken/load.py:52\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     54\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Load the Tokenizer using the Fast class directly\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Tokenizer (Direct Fast Class)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Manually set the chat template/padding if needed (common for Llama-3)\u001b[39;00m\n\u001b[32m     14\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2314\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, PreTrainedTokenizerFast\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "# 1. Load the Tokenizer using the Fast class directly\n",
    "print(\"Step 1: Loading Tokenizer (Direct Fast Class)...\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_path, \n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Manually set the chat template/padding if needed (common for Llama-3)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Load the Model\n",
    "print(\"Step 2: Loading Model weights...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\", # Automatically chooses bfloat16 or float16\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SUCCESS! The model and tokenizer are loaded and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25e8aca2-21eb-476b-9675-a2af33ec35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: tokenizer.json is missing! You must copy it from the cache.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "# 1. Force tokenizer_config.json to use the Fast class\n",
    "config_path = os.path.join(model_path, \"tokenizer_config.json\")\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    data[\"tokenizer_class\"] = \"PreTrainedTokenizerFast\"\n",
    "    # Remove references to the slow tokenizer file\n",
    "    data.pop(\"tokenizer_file\", None) \n",
    "    \n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(\"‚úÖ Fixed tokenizer_config.json\")\n",
    "\n",
    "# 2. Ensure we have a valid tokenizer.json\n",
    "if not os.path.exists(os.path.join(model_path, \"tokenizer.json\")):\n",
    "    print(\"‚ùå Error: tokenizer.json is missing! You must copy it from the cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a7e6b81-2dda-4bbc-a880-0e43d319c152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Tokenizer via direct JSON map...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m./hf_models/model_1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Tokenizer via direct JSON map...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokenizer = \u001b[43mPreTrainedTokenizerFast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/tokenizer.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|begin_of_text|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Standard Llama-3 tokens\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|end_of_text|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Loading Model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     15\u001b[39m     model_path,\n\u001b[32m     16\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m     17\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_fast.py:117\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m     fast_tokenizer = copy.deepcopy(tokenizer_object)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     fast_tokenizer = \u001b[43mTokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[32m    120\u001b[39m     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[31mException\u001b[39m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast\n",
    "import torch\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Step 1: Loading Tokenizer via direct JSON map...\")\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=f\"{model_path}/tokenizer.json\",\n",
    "    bos_token=\"<|begin_of_text|>\", # Standard Llama-3 tokens\n",
    "    eos_token=\"<|end_of_text|>\",\n",
    ")\n",
    "\n",
    "print(\"Step 2: Loading Model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SUCCESS! The model is finally loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f79e4460-a05b-43fa-abe2-267412932997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of ./hf_models/model_1: ['config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"./hf_models/model_1\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    print(f\"Contents of {path}: {os.listdir(path)}\")\n",
    "else:\n",
    "    print(f\"‚ùå The directory {path} does not even exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae90183b-67f1-4e7f-8e1f-92deca65eef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/tokenizer.json': No such file or directory\n",
      "cp: cannot stat '/root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/*.safetensors': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 1. Re-create the folder just in case\n",
    "!mkdir -p ./hf_models/model_1\n",
    "\n",
    "# 2. Copy the tokenizer and the weights directly from the cache\n",
    "# Note: I am using the exact path from your previous screenshot\n",
    "!cp -L /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/tokenizer.json ./hf_models/model_1/\n",
    "!cp -L /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/config.json ./hf_models/model_1/\n",
    "!cp -L /root/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/*.safetensors ./hf_models/model_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10a2ea88-395b-43a5-8f27-272d6eca0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Tokenizer...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Loading Tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We use use_fast=True because Llama-3 depends on the 'tokenizer.json' file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Loading Model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     12\u001b[39m     model_path,\n\u001b[32m     13\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m     14\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1078\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py:1321\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1313\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1319\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1320\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1324\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING.keys(), key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:808\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    806\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    811\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/configuration_llama.py:209\u001b[39m, in \u001b[36mLlamaConfig.__init__\u001b[39m\u001b[34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mlp_bias, head_dim, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_dropout = attention_dropout\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp_bias = mlp_bias\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28mself\u001b[39m.head_dim = head_dim \u001b[38;5;28;01mif\u001b[39;00m head_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Validate the correctness of rotary position embeddings parameters\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# BC: if there is a 'type' field, copy it it to 'rope_type'.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling:\n",
      "\u001b[31mZeroDivisionError\u001b[39m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Step 1: Loading Tokenizer...\")\n",
    "# We use use_fast=True because Llama-3 depends on the 'tokenizer.json' file\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True, use_fast=True)\n",
    "\n",
    "print(\"Step 2: Loading Model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DONE! The model is ready for your questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "217aec59-bd26-46c8-be21-3533e3adc658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating a manual configuration to bypass file errors...\n",
      "Step 2: Loading Tokenizer...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2: Loading Tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# We pass the config directly so it doesn't have to divide by zero while reading files\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanual_config\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 3: Loading Model weights...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     26\u001b[39m     model_path,\n\u001b[32m     27\u001b[39m     config=manual_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2108\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m     slow_tokenizer = \u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslow_tokenizer_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2120\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2174\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2172\u001b[39m \u001b[38;5;66;03m# Second attempt. If we have not yet found tokenizer_class, let's try to use the config.\u001b[39;00m\n\u001b[32m   2173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2174\u001b[39m     config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2180\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2182\u001b[39m     config_tokenizer_class = config.tokenizer_class\n\u001b[32m   2183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[32m   2184\u001b[39m     \u001b[38;5;66;03m# skip if an error occurred.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py:1321\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1313\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1319\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1320\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1324\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING.keys(), key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:808\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    806\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    811\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/configuration_llama.py:209\u001b[39m, in \u001b[36mLlamaConfig.__init__\u001b[39m\u001b[34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mlp_bias, head_dim, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_dropout = attention_dropout\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp_bias = mlp_bias\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28mself\u001b[39m.head_dim = head_dim \u001b[38;5;28;01mif\u001b[39;00m head_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Validate the correctness of rotary position embeddings parameters\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# BC: if there is a 'type' field, copy it it to 'rope_type'.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_scaling:\n",
      "\u001b[31mZeroDivisionError\u001b[39m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./hf_models/model_1\"\n",
    "\n",
    "print(\"Step 1: Creating a manual configuration to bypass file errors...\")\n",
    "# These are standard Llama-3-8B values. \n",
    "# If you are using a different size, let me know!\n",
    "manual_config = LlamaConfig(\n",
    "    hidden_size=4096,\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    model_type=\"llama\"\n",
    ")\n",
    "\n",
    "print(\"Step 2: Loading Tokenizer...\")\n",
    "# We pass the config directly so it doesn't have to divide by zero while reading files\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    local_files_only=True, \n",
    "    config=manual_config\n",
    ")\n",
    "\n",
    "print(\"Step 3: Loading Model weights...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=manual_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SUCCESS! The ZeroDivisionError has been bypassed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb36f1f7-2970-496c-a2de-5e2d3fd5893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Mistral Configuration...\n",
      "Step 2: Loading Mistral Tokenizer...\n",
      "Step 3: Loading Mistral Model (this requires ~15GB VRAM or 4-bit)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find MistralForCausalLM neither in <module 'transformers.models.mistral' from '/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:741\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:745\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find MistralForCausalLM in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     13\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     14\u001b[39m     model_path, \n\u001b[32m     15\u001b[39m     local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     16\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 3: Loading Mistral Model (this requires ~15GB VRAM or 4-bit)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Mistral-7B-v0.3 Loaded Successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:601\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    598\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:394\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:807\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    806\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    810\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:821\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    820\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:743\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find MistralForCausalLM neither in <module 'transformers.models.mistral' from '/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Your new specific path\n",
    "model_path = \"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"\n",
    "\n",
    "print(\"Step 1: Loading Mistral Configuration...\")\n",
    "# Mistral v0.3 has specific settings, we load from the folder to be safe\n",
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "print(\"Step 2: Loading Mistral Tokenizer...\")\n",
    "# We use trust_remote_code because Mistral v3 often requires it for its custom tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Step 3: Loading Mistral Model (this requires ~15GB VRAM or 4-bit)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mistral-7B-v0.3 Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543461d8-de29-49ff-bad4-80fbc2d23316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate(self, prompt):\n",
    "    # Mistral v0.3 prompt format\n",
    "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "    inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "    outputs = self.model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=self.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the instruction part to get just the answer\n",
    "    return decoded.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1af77d9c-8eef-455c-beef-f5b50701f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: 15\n",
      "‚úÖ Weights found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(model_path)\n",
    "print(f\"Files found: {len(files)}\")\n",
    "if \"model.safetensors.index.json\" in files or \"model.safetensors\" in files:\n",
    "    print(\"‚úÖ Weights found.\")\n",
    "else:\n",
    "    print(\"‚ùå Weights missing! Run the 'cp -L' command for this new path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f07845a-2f6d-40aa-91eb-d6d27319db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agents/question_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_agent.py\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class QuestionAgent:\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Initializing Mistral Agent from {model_path}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _clean_json(self, text):\n",
    "        \"\"\"Extracts JSON structure from model chatter.\"\"\"\n",
    "        try:\n",
    "            match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            return match.group(0) if match else text\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def _get_single_vote(self, prompt):\n",
    "        \"\"\"Single inference pass.\"\"\"\n",
    "        formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.8, # Higher temp for diverse voting\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        raw_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        json_str = self._clean_json(raw_output.split(\"[/INST]\")[-1])\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            return data, generation_time\n",
    "        except:\n",
    "            return None, generation_time\n",
    "\n",
    "    def generate_with_voting(self, domain, num_votes=3):\n",
    "        \"\"\"Implements Multi-vote, Confidence Scoring, and Adaptive Voting.\"\"\"\n",
    "        prompt = f\"Create a difficult multiple choice question about {domain} in JSON format with keys: 'question', 'choices', 'answer' (A, B, C, or D).\"\n",
    "        \n",
    "        votes = []\n",
    "        times = []\n",
    "        \n",
    "        for i in range(num_votes):\n",
    "            res, duration = self._get_single_vote(prompt)\n",
    "            if res:\n",
    "                votes.append(res)\n",
    "                times.append(duration)\n",
    "\n",
    "        if not votes:\n",
    "            return None\n",
    "\n",
    "        # --- Adaptive Voting Logic ---\n",
    "        answers = [v['answer'] for v in votes]\n",
    "        vote_counts = Counter(answers)\n",
    "        winning_answer = vote_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Confidence Score: Agreement ratio\n",
    "        confidence = vote_counts[winning_answer] / len(votes)\n",
    "        \n",
    "        # Select a representative sample that matches the winning answer\n",
    "        final_question = next(v for v in votes if v['answer'] == winning_answer)\n",
    "        final_question['metadata'] = {\n",
    "            \"confidence_score\": confidence,\n",
    "            \"avg_generation_time\": sum(times) / len(times),\n",
    "            \"vote_distribution\": dict(vote_counts)\n",
    "        }\n",
    "        \n",
    "        return final_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05d5821-5bf8-4ce3-bd20-64be5f1f05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mistral Agent from /root/.cache/huggingface/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find Qwen3ForCausalLM neither in <module 'transformers.models.qwen3' from '/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:741\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:745\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find Qwen3ForCausalLM in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# agent = QuestionAgent(\"/workspace/AAIPL_10.108.10.21/hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# agent = QuestionAgent(\"/root/.cache/huggingface/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m agent = \u001b[43mQuestionAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/root/.cache/huggingface/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m domains = [\u001b[33m\"\u001b[39m\u001b[33mQuantum Mechanics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCell Biology\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMacroeconomics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mOrganic Chemistry\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m dataset_file = \u001b[33m\"\u001b[39m\u001b[33msynthetic_quiz_dataset.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AAIPL_10.108.10.21/agents/question_agent.py:12\u001b[39m, in \u001b[36mQuestionAgent.__init__\u001b[39m\u001b[34m(self, model_path)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitializing Mistral Agent from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:601\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    598\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:394\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:807\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    806\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    810\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:821\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    820\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py:743\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find Qwen3ForCausalLM neither in <module 'transformers.models.qwen3' from '/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!"
     ]
    }
   ],
   "source": [
    "from agents.question_agent import QuestionAgent\n",
    "import json\n",
    "\n",
    "# Initialize\n",
    "# agent = QuestionAgent(\"/workspace/AAIPL_10.108.10.21/hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\")\n",
    "# agent = QuestionAgent(\"/root/.cache/huggingface/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\")\n",
    "agent = QuestionAgent(\"/root/.cache/huggingface/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c\")\n",
    "\n",
    "domains = [\"Quantum Mechanics\", \"Cell Biology\", \"Macroeconomics\", \"Organic Chemistry\"]\n",
    "dataset_file = \"synthetic_quiz_dataset.jsonl\"\n",
    "\n",
    "print(f\"Starting dataset generation to {dataset_file}...\")\n",
    "\n",
    "with open(dataset_file, \"a\") as f:\n",
    "    for domain in domains:\n",
    "        print(f\"Generating for {domain}...\")\n",
    "        # Running with 3 votes for consensus\n",
    "        final_data = agent.generate_with_voting(domain, num_votes=3)\n",
    "        \n",
    "        if final_data:\n",
    "            f.write(json.dumps(final_data) + \"\\n\")\n",
    "            print(f\"‚úÖ Success. Confidence: {final_data['metadata']['confidence_score']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to generate valid JSON for {domain}\")\n",
    "\n",
    "print(\"Dataset generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2298cd9-6a6a-47cd-9a85-d5193a1b1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30b327f8-c408-41d2-a975-030d27c71a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_transfer                       0.1.9\n",
      "s3transfer                        0.14.0\n",
      "transformers                      4.56.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350d8349-97e6-4127-a50b-c55a93646c77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to compare versions for packaging>=20.0: need=20.0 found=None. This is unusual. Consider reinstalling packaging.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     is_pretty_midi_available,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py:57\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m     55\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, check dependency_versions_table.py\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py:117\u001b[39m, in \u001b[36mrequire_version_core\u001b[39m\u001b[34m(requirement)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m hint = \u001b[33m\"\u001b[39m\u001b[33mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.[dev]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m` if you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre working with git main\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py:111\u001b[39m, in \u001b[36mrequire_version\u001b[39m\u001b[34m(requirement, hint)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted.items():\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py:39\u001b[39m, in \u001b[36m_compare_versions\u001b[39m\u001b[34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compare_versions\u001b[39m(op, got_ver, want_ver, requirement, pkg, hint):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m got_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     40\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is unusual. Consider\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m         )\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version.parse(got_ver), version.parse(want_ver)):\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     45\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Unable to compare versions for packaging>=20.0: need=20.0 found=None. This is unusual. Consider reinstalling packaging."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931049f4-c1aa-4991-9bd1-2ed7b6290b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers requests\n",
    "# !pip install requests==2.32.3 transformers==4.56.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3dccef-d3d7-4a58-9e5a-8709afae8ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep trans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
