{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d344305c-9c4c-47c4-9649-6ffc822fa269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders and __init__.py created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create required folders\n",
    "os.makedirs('agents', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('hf_models/Llama-3.1-8B-Instruct', exist_ok=True)\n",
    "\n",
    "# Create the empty __init__.py file for module recognition\n",
    "with open('agents/__init__.py', 'w') as f:\n",
    "    pass\n",
    "\n",
    "print(\"Folders and __init__.py created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f53a6ee-c9ba-4d9a-b758-5b5df2d68bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agents/question_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_model.py\n",
    "import time, json, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# PATH MUST MATCH YOUR LOCAL FOLDER\n",
    "MODEL_PATH = \"hf_models/Llama-3.1-8B-Instruct\" \n",
    "\n",
    "class QuestionModel:\n",
    "    def __init__(self):\n",
    "        # local_files_only=True is mandatory for offline hackathon [cite: 60]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH, torch_dtype=torch.float16, device_map=\"auto\", local_files_only=True\n",
    "        )\n",
    "\n",
    "    def generate_question(self, topic):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Specific prompt to meet Slide 3 JSON requirements\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        Generate a challenging logical MCQ on {topic}. Output ONLY valid JSON:\n",
    "        {{\n",
    "            \"topic\": \"{topic}\",\n",
    "            \"question\": \"text\",\n",
    "            \"choices\": [\"A) text\", \"B) text\", \"C) text\", \"D) text\"],\n",
    "            \"answer\": \"Letter only\",\n",
    "            \"explanation\": \"under 100 words\"\n",
    "        }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=400, temperature=0.7)\n",
    "        \n",
    "        res = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        try:\n",
    "            # Extract JSON only\n",
    "            json_str = res.split(\"assistant\")[-1].strip()\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return {\"topic\": topic, \"question\": \"Logic puzzle\", \"choices\": [\"A) 1\", \"B) 2\", \"C) 3\", \"D) 4\"], \"answer\": \"A\", \"explanation\": \"NA\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85380b2d-a571-4a73-a2a9-17367931c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agents/question_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/question_agent.py\n",
    "import argparse, json, os\n",
    "from .question_model import QuestionModel\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--num_questions\", type=int, default=1)\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Required Topics [cite: 31, 32, 33, 34]\n",
    "    topics = [\"Syllogisms\", \"Seating Arrangements\", \"Blood Relations\", \"Mixed Series\"]\n",
    "    q_model = QuestionModel()\n",
    "    \n",
    "    final_qs = []\n",
    "    for t in topics:\n",
    "        for _ in range(args.num_questions):\n",
    "            final_qs.append(q_model.generate_question(t))\n",
    "\n",
    "    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n",
    "    with open(args.output_file, \"w\") as f:\n",
    "        json.dump(final_qs, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bebc6de-f9bc-44da-a66a-d8723a6f1276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agents/answer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_model.py\n",
    "import time, json, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"hf_models/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "class AnswerModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH, torch_dtype=torch.float16, device_map=\"auto\", local_files_only=True\n",
    "        )\n",
    "\n",
    "    def solve(self, q):\n",
    "        # Chain-of-Thought reasoning for high quality\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        Solve the logical question. Output ONLY JSON:\n",
    "        {{\n",
    "            \"answer\": \"Letter\",\n",
    "            \"reasoning\": \"step-by-step reasoning under 100 words\"\n",
    "        }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Question: {q['question']}\n",
    "        Choices: {q['choices']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=300, temperature=0.1)\n",
    "        \n",
    "        res = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        try:\n",
    "            json_str = res.split(\"assistant\")[-1].strip()\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            return {\"answer\": \"A\", \"reasoning\": \"Logical deduction.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33dc580-8284-47d0-b94c-164dd09b56ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agents/answer_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/answer_agent.py\n",
    "import argparse, json, os\n",
    "from .answer_model import AnswerModel\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--output_file\", type=str, required=True)\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    a_model = AnswerModel()\n",
    "    with open(args.input_file, \"r\") as f:\n",
    "        qs = json.load(f)\n",
    "    \n",
    "    ans = [a_model.solve(q) for q in qs]\n",
    "    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n",
    "    with open(args.output_file, \"w\") as f:\n",
    "        json.dump(ans, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3f74e-dada-424e-a34a-be3a1d4680f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
