{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e61dbb5-e234-45bf-ba81-f5a4f08439c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating environment for Transformers 4.56.2...\n",
      "Found existing installation: transformers 4.56.2\n",
      "Uninstalling transformers-4.56.2:\n",
      "  Successfully uninstalled transformers-4.56.2\n",
      "Found existing installation: requests 2.32.3\n",
      "Uninstalling requests-2.32.3:\n",
      "  Successfully uninstalled requests-2.32.3\n",
      "Found existing installation: tokenizers 0.22.1\n",
      "Uninstalling tokenizers-0.22.1:\n",
      "  Successfully uninstalled tokenizers-0.22.1\n",
      "Found existing installation: accelerate 1.11.0\n",
      "Uninstalling accelerate-1.11.0:\n",
      "  Successfully uninstalled accelerate-1.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting requests==2.32.3\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting transformers==4.56.2\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tokenizers==0.21.0\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting accelerate==1.1.0\n",
      "  Downloading accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2026.1.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.10.23)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install tokenizers==0.21.0 and transformers==4.56.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested tokenizers==0.21.0\n",
      "    transformers 4.56.2 depends on tokenizers<=0.23.0 and >=0.22.0\n",
      "\n",
      "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
      "    tokenizers\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mWriting agents/question_agent.py...\n",
      "Loading Model... this may take a minute.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 4. EXECUTE IMMEDIATELY\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Model... this may take a minute.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquestion_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionAgent\n\u001b[32m     92\u001b[39m MODEL_PATH = \u001b[33m\"\u001b[39m\u001b[33mhf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/AAIPLTeam43/agents/question_agent.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mQuestionAgent\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# 1. CLEAN INSTALLATION (Fixes the \"requests\" metadata error)\n",
    "print(\"Updating environment for Transformers 4.56.2...\")\n",
    "!pip uninstall -y transformers requests tokenizers accelerate\n",
    "!pip install requests==2.32.3 transformers==4.56.2 tokenizers==0.21.0 accelerate==1.1.0 bitsandbytes==0.44.1\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 2. CREATE DIRECTORY\n",
    "os.makedirs(\"agents\", exist_ok=True)\n",
    "\n",
    "# 3. WRITE THE AGENT FILE (%%writefile logic)\n",
    "print(\"Writing agents/question_agent.py...\")\n",
    "with open(\"agents/question_agent.py\", \"w\") as f:\n",
    "    f.write('''\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class QuestionAgent:\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Initializing Mistral v0.3 from: {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        \"\"\"Fixes the .index('{') error by using robust regex matching.\"\"\"\n",
    "        try:\n",
    "            # This looks for the content between the first { and last }\n",
    "            match = re.search(r'\\\\{.*\\\\}', text, re.DOTALL)\n",
    "            if match:\n",
    "                return json.loads(match.group(0))\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _get_vote(self, domain):\n",
    "        prompt = f\"<s>[INST] Create a difficult MCQ about {domain} in JSON format. Keys: 'question', 'choices' (A,B,C,D), 'answer'. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=512, temperature=0.8, do_sample=True)\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        raw_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract everything after the Instruction tag\n",
    "        clean_content = raw_output.split(\"[/INST]\")[-1]\n",
    "        \n",
    "        return self._extract_json(clean_content), latency\n",
    "\n",
    "    def generate_with_voting(self, domain, num_votes=3):\n",
    "        votes, times = [], []\n",
    "        for _ in range(num_votes):\n",
    "            data, t = self._get_vote(domain)\n",
    "            if data:\n",
    "                votes.append(data)\n",
    "                times.append(t)\n",
    "        \n",
    "        if not votes: return None\n",
    "\n",
    "        # Adaptive Voting logic\n",
    "        answers = [v.get('answer', 'A') for v in votes]\n",
    "        winner = Counter(answers).most_common(1)[0][0]\n",
    "        \n",
    "        final_question = next((v for v in votes if v.get('answer') == winner), votes[0])\n",
    "        final_question['metadata'] = {\n",
    "            \"confidence\": Counter(answers)[winner] / len(votes),\n",
    "            \"avg_latency\": sum(times) / len(times)\n",
    "        }\n",
    "        return final_question\n",
    "''')\n",
    "\n",
    "# 4. EXECUTE IMMEDIATELY\n",
    "print(\"Loading Model... this may take a minute.\")\n",
    "from agents.question_agent import QuestionAgent\n",
    "\n",
    "MODEL_PATH = \"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"\n",
    "\n",
    "try:\n",
    "    agent = QuestionAgent(MODEL_PATH)\n",
    "    \n",
    "    # Test it\n",
    "    test_domain = \"Quantum Cryptography\"\n",
    "    print(f\"Generating consensus for: {test_domain}\")\n",
    "    result = agent.generate_with_voting(test_domain, num_votes=3)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\\\n✅ SUCCESS!\")\n",
    "        print(json.dumps(result, indent=4))\n",
    "except Exception as e:\n",
    "    print(f\"\\\\n❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5a567-8173-4552-be24-569fad0fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b78695-f837-44f4-88e2-480ebc418222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.32.3\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting transformers==4.56.2\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tokenizers==0.21.0\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting accelerate==1.1.0\n",
      "  Using cached accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2026.1.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.10.23)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install tokenizers==0.21.0 and transformers==4.56.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested tokenizers==0.21.0\n",
      "    transformers 4.56.2 depends on tokenizers<=0.23.0 and >=0.22.0\n",
      "\n",
      "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
      "    tokenizers\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Force the installation of the specific ecosystem\n",
    "!pip install --upgrade requests==2.32.3 transformers==4.56.2 tokenizers==0.21.0 accelerate==1.1.0 bitsandbytes==0.44.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98622bee-055c-4f1d-9e3c-f7b3ff52bf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping requests as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tokenizers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: bitsandbytes 0.48.2.dev0\n",
      "Uninstalling bitsandbytes-0.48.2.dev0:\n",
      "  Successfully uninstalled bitsandbytes-0.48.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting requests==2.32.3\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting transformers==4.56.2\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tokenizers==0.22.2\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting accelerate==1.1.0\n",
      "  Using cached accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2026.1.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.10.23)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.1.0) (7.1.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.1.0) (2.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2026.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==1.1.0) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=1.10.0->accelerate==1.1.0) (1.3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==1.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==1.1.0) (3.0.3)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.1.0-py3-none-any.whl (333 kB)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: requests, tokenizers, transformers, bitsandbytes, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [accelerate]5\u001b[0m [accelerate]s]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.10.10 requires cut_cross_entropy, which is not installed.\n",
      "unsloth-zoo 2025.10.10 requires torchao>=0.13.0, which is not installed.\n",
      "trl 0.23.0 requires accelerate>=1.4.0, but you have accelerate 1.1.0 which is incompatible.\n",
      "datasets 4.3.0 requires fsspec[http]<=2025.9.0,>=2023.1.0, but you have fsspec 2026.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.1.0 bitsandbytes-0.44.1 requests-2.32.3 tokenizers-0.22.2 transformers-4.56.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Force a clean install with the exact dependency tree required\n",
    "!pip uninstall -y transformers requests tokenizers accelerate bitsandbytes\n",
    "!pip install requests==2.32.3 \\\n",
    "             transformers==4.56.2 \\\n",
    "             tokenizers==0.22.2 \\\n",
    "             accelerate==1.1.0 \\\n",
    "             bitsandbytes==0.44.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860fb438-e32f-4cf7-ac82-681bc9c060b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.56.2\n",
      "Uninstalling transformers-4.56.2:\n",
      "  Successfully uninstalled transformers-4.56.2\n",
      "Found existing installation: requests 2.32.3\n",
      "Uninstalling requests-2.32.3:\n",
      "  Successfully uninstalled requests-2.32.3\n",
      "Found existing installation: tokenizers 0.22.2\n",
      "Uninstalling tokenizers-0.22.2:\n",
      "  Successfully uninstalled tokenizers-0.22.2\n",
      "Found existing installation: accelerate 1.1.0\n",
      "Uninstalling accelerate-1.1.0:\n",
      "  Successfully uninstalled accelerate-1.1.0\n",
      "Found existing installation: bitsandbytes 0.44.1\n",
      "Uninstalling bitsandbytes-0.44.1:\n",
      "  Successfully uninstalled bitsandbytes-0.44.1\n",
      "Found existing installation: fsspec 2026.2.0\n",
      "Uninstalling fsspec-2026.2.0:\n",
      "  Successfully uninstalled fsspec-2026.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting requests==2.32.3\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting transformers==4.56.2\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting tokenizers==0.22.2\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting accelerate==1.4.0\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec==2025.9.0\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting bitsandbytes==0.44.1\n",
      "  Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2026.1.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.10.23)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0) (7.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate==1.4.0) (1.3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.4.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.4.0) (3.0.3)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "Installing collected packages: requests, fsspec, tokenizers, transformers, bitsandbytes, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [accelerate]6\u001b[0m [bitsandbytes]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.10.10 requires cut_cross_entropy, which is not installed.\n",
      "unsloth-zoo 2025.10.10 requires torchao>=0.13.0, which is not installed.\n",
      "torchaudio 2.2.2+rocm5.6 requires torch==2.2.2, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.4.0 bitsandbytes-0.44.1 fsspec-2025.9.0 requests-2.32.3 tokenizers-0.22.2 transformers-4.56.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. Clean out the conflicting versions\n",
    "!pip uninstall -y transformers requests tokenizers accelerate bitsandbytes fsspec\n",
    "\n",
    "# 2. Install the 'Golden Trio' that satisfies Mistral + TRL + Datasets\n",
    "!pip install requests==2.32.3 \\\n",
    "             transformers==4.56.2 \\\n",
    "             tokenizers==0.22.2 \\\n",
    "             accelerate==1.4.0 \\\n",
    "             fsspec==2025.9.0 \\\n",
    "             bitsandbytes==0.44.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9edbe8-e1b3-4ee4-9128-8164a3c0d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating agents/question_agent.py with robust JSON extraction...\n",
      "\n",
      "--- Model Setup Starting ---\n",
      "Initializing Mistral v0.3 from: hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\n",
      "\n",
      "❌ Execution Error: Could not import module 'MistralForCausalLM'. Are this object's requirements defined correctly?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Ensure the agents directory exists\n",
    "os.makedirs(\"agents\", exist_ok=True)\n",
    "\n",
    "# 2. WRITE/OVERWRITE the agent file with the Regex Fix\n",
    "print(\"Updating agents/question_agent.py with robust JSON extraction...\")\n",
    "with open(\"agents/question_agent.py\", \"w\") as f:\n",
    "    f.write('''\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "class QuestionAgent:\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Initializing Mistral v0.3 from: {model_path}\")\n",
    "        \n",
    "        # 4-bit Quantization to fit in GPU memory\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        \"\"\"Robustly finds JSON using regex. Fixes the .index('{') error.\"\"\"\n",
    "        try:\n",
    "            # Searches for everything between the first '{' and last '}'\n",
    "            match = re.search(r'\\\\{.*\\\\}', text, re.DOTALL)\n",
    "            if match:\n",
    "                return json.loads(match.group(0))\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"  [Extraction Error]: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_vote(self, domain):\n",
    "        \"\"\"Standard Mistral-v0.3 Prompting Format\"\"\"\n",
    "        prompt = f\"<s>[INST] Create a difficult MCQ about {domain} in valid JSON. Keys: 'question', 'choices' (A,B,C,D), 'answer'. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=512, \n",
    "                temperature=0.8, \n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        duration = time.time() - start\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        clean_text = response.split(\"[/INST]\")[-1].strip()\n",
    "        \n",
    "        return self._extract_json(clean_text), duration\n",
    "\n",
    "    def generate_consensus(self, domain, num_votes=3):\n",
    "        \"\"\"Adaptive Voting logic to ensure high accuracy.\"\"\"\n",
    "        votes = []\n",
    "        latencies = []\n",
    "        for i in range(num_votes):\n",
    "            print(f\"  - Attempting generation {i+1}...\")\n",
    "            data, t = self._get_vote(domain)\n",
    "            if data:\n",
    "                votes.append(data)\n",
    "                latencies.append(t)\n",
    "        \n",
    "        if not votes: return None\n",
    "\n",
    "        # Voting Logic: Find the most common answer\n",
    "        answers = [v.get('answer', 'A') for v in votes]\n",
    "        winner = Counter(answers).most_common(1)[0][0]\n",
    "        \n",
    "        # Pick the question object that provided the winning answer\n",
    "        final_data = next((v for v in votes if v.get('answer') == winner), votes[0])\n",
    "        final_data['metadata'] = {\n",
    "            \"confidence\": round(Counter(answers)[winner] / len(votes), 2),\n",
    "            \"avg_latency\": round(sum(latencies) / len(latencies), 2)\n",
    "        }\n",
    "        return final_data\n",
    "''')\n",
    "\n",
    "# 3. INITIALIZE AND TEST\n",
    "from agents.question_agent import QuestionAgent\n",
    "\n",
    "MODEL_PATH = \"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- Model Setup Starting ---\")\n",
    "    agent = QuestionAgent(MODEL_PATH)\n",
    "    \n",
    "    topic = \"Quantum Computing\"\n",
    "    print(f\"\\nGenerating entry for: {topic}\")\n",
    "    result = agent.generate_consensus(topic, num_votes=3)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n✅ SUCCESS: Question generated with high confidence.\")\n",
    "        print(json.dumps(result, indent=4))\n",
    "        \n",
    "        # Save to file\n",
    "        with open(\"quiz_dataset.jsonl\", \"a\") as f:\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "    else:\n",
    "        print(\"\\n❌ Failed to generate a valid consensus question.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Execution Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3fe66e-b232-47b9-8eac-b8d624e8f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mistral v0.3 from: hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\n",
      "\n",
      "❌ Still hitting an error: Could not find MistralForCausalLM neither in <module 'transformers.models.mistral' from '/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Overwrite with the 'AutoModel' bypass logic\n",
    "with open(\"agents/question_agent.py\", \"w\") as f:\n",
    "    f.write('''\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "class QuestionAgent:\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"--- Bypassing Registry: Loading Mistral via AutoModel ---\")\n",
    "        \n",
    "        # This config is essential for the 4.56.2 + Mistral v0.3 combo\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            local_files_only=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Using trust_remote_code=True solves the 'Could not import MistralForCausalLM' error\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        try:\n",
    "            match = re.search(r'\\\\{.*\\\\}', text, re.DOTALL)\n",
    "            return json.loads(match.group(0)) if match else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _get_vote(self, domain):\n",
    "        prompt = f\"<s>[INST] Create a difficult MCQ about {domain} in valid JSON. Keys: 'question', 'choices' (A,B,C,D), 'answer'. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=512, \n",
    "                temperature=0.7, \n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        clean_text = response.split(\"[/INST]\")[-1].strip()\n",
    "        return self._extract_json(clean_text)\n",
    "\n",
    "    def generate_consensus(self, domain, num_votes=3):\n",
    "        votes = []\n",
    "        for i in range(num_votes):\n",
    "            print(f\"  - Vote {i+1}...\")\n",
    "            data = self._get_vote(domain)\n",
    "            if data: votes.append(data)\n",
    "        \n",
    "        if not votes: return None\n",
    "\n",
    "        answers = [v.get('answer', 'A') for v in votes]\n",
    "        winner = Counter(answers).most_common(1)[0][0]\n",
    "        final_data = next((v for v in votes if v.get('answer') == winner), votes[0])\n",
    "        final_data['confidence'] = round(Counter(answers)[winner] / len(votes), 2)\n",
    "        return final_data\n",
    "''')\n",
    "\n",
    "# 2. Re-import and Run\n",
    "from agents.question_agent import QuestionAgent\n",
    "\n",
    "MODEL_PATH = \"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"\n",
    "\n",
    "try:\n",
    "    agent = QuestionAgent(MODEL_PATH)\n",
    "    result = agent.generate_consensus(\"Quantum Physics\", num_votes=3)\n",
    "    if result:\n",
    "        print(\"\\n✅ SUCCESS!\")\n",
    "        print(json.dumps(result, indent=4))\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Still hitting an error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5633a2ea-54b5-4a01-92ed-dae0529cda7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriting agent with Direct-Class mapping...\n",
      "Initializing Mistral v0.3 from: hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\n",
      "\n",
      "❌ Final Registry Error Check: Could not find MistralForCausalLM neither in <module 'transformers.models.mistral' from '/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!\n",
      "\n",
      "Tip: If this still fails, your snapshot folder might be missing the 'configuration_mistral.py' or 'modeling_mistral.py' files required for remote code execution.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Update the agent file with the Force-Load logic\n",
    "print(\"Rewriting agent with Direct-Class mapping...\")\n",
    "with open(\"agents/question_agent.py\", \"w\") as f:\n",
    "    f.write('''\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "class QuestionAgent:\n",
    "    def __init__(self, model_path):\n",
    "        print(f\"--- Force Loading Mistral Architecture ---\")\n",
    "        \n",
    "        # 4-bit config for Mistral-7B\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, \n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        # We use AutoModelForCausalLM but force the loading via the snapshot path\n",
    "        # trust_remote_code=True is essential here to allow the config.json \n",
    "        # to define the model architecture manually.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            local_files_only=True,\n",
    "            trust_remote_code=True,\n",
    "            revision=\"main\"\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _extract_json(self, text):\n",
    "        try:\n",
    "            match = re.search(r'\\\\{.*\\\\}', text, re.DOTALL)\n",
    "            return json.loads(match.group(0)) if match else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _get_vote(self, domain):\n",
    "        prompt = f\"<s>[INST] Create a difficult MCQ about {domain} in JSON format. Keys: 'question', 'choices', 'answer'. [/INST]\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=512, \n",
    "                temperature=0.7, \n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        clean_text = response.split(\"[/INST]\")[-1].strip()\n",
    "        return self._extract_json(clean_text)\n",
    "\n",
    "    def generate_consensus(self, domain, num_votes=3):\n",
    "        votes = []\n",
    "        for i in range(num_votes):\n",
    "            print(f\"  - Attempt {i+1}...\")\n",
    "            data = self._get_vote(domain)\n",
    "            if data: votes.append(data)\n",
    "        \n",
    "        if not votes: return None\n",
    "\n",
    "        answers = [v.get('answer', 'A') for v in votes]\n",
    "        winner = Counter(answers).most_common(1)[0][0]\n",
    "        final_data = next((v for v in votes if v.get('answer') == winner), votes[0])\n",
    "        return final_data\n",
    "''')\n",
    "\n",
    "# 2. Re-import and Execute\n",
    "from agents.question_agent import QuestionAgent\n",
    "\n",
    "MODEL_PATH = \"hf_models/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71\"\n",
    "\n",
    "try:\n",
    "    # Explicitly clear CUDA cache before loading to prevent VRAM fragmentation\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    agent = QuestionAgent(MODEL_PATH)\n",
    "    print(\"\\n✅ Initialization successful. Testing generation...\")\n",
    "    \n",
    "    result = agent.generate_consensus(\"Cybersecurity\", num_votes=2)\n",
    "    if result:\n",
    "        print(\"\\n--- Final Question ---\")\n",
    "        print(json.dumps(result, indent=4))\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Final Registry Error Check: {e}\")\n",
    "    print(\"\\nTip: If this still fails, your snapshot folder might be missing the 'configuration_mistral.py' or 'modeling_mistral.py' files required for remote code execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dfc255-f673-4732-8a48-484144bf21d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
